% ramalama-client 1

## NAME
ramalama\-client - interact with an OpenAI-compatible API endpoint (experimental)

## SYNOPSIS
**ramalama client** [*options*] _host_ [_args_...]

## OPTIONS

#### **--help**, **-h**
show this help message and exit

#### **--rag** _path_
RAG vector database or OCI Image to be served with the model

#### **--model** _model_
model to use for chat completions (default: gpt-3.5-turbo)

#### **--api-key** _key_
API key for the OpenAI API. Can also be specified via LLM_API_KEY environment variable

#### **--network**, **--net** _mode_
set the network mode for the container (default: bridge)

#### **--container**
run RamaLama in the default container

#### **--nocontainer**
do not run RamaLama in the default container

#### **--image** _image_
OCI container image to run with the specified AI model

## DESCRIPTION
Interact with an OpenAI-compatible API endpoint. The client can send queries to the API and retrieve responses. When used with the --rag option, it enables retrieval augmented generation capabilities.

## EXAMPLES

### Connect to an OpenAI-compatible API endpoint
```
$ ramalama client http://127.0.0.1:8080
```

### Use with RAG capabilities
```
$ ramalama client --rag my_knowledge_base.db http://127.0.0.1:8080
```

### Send a one-shot query without entering interactive mode
```
$ ramalama client http://127.0.0.1:8080 "What is the capital of France?"
```

### Use remote API with a RAG image generated by ramalama-rag

```
$ ramalama client --rag localhost/myrag:latest --api-key "API_KEY" --model "openai/gpt-4o-mini" https://openrouter.ai/api/v1
```

## SEE ALSO
**[ramalama(1)](ramalama.1.md)**, **[ramalama-serve(1)](ramalama-serve.1.md)**, **[ramalama-rag(1)](ramalama-rag.1.md)**

## HISTORY
Apr 2025, Originally compiled by Eric Curtin <ecurtin@redhat.com>
