# The RamaLama configuration file specifies all of the available configuration
# command-line options/flags for container engine tools like Podman & Buildah,
# but in a TOML format that can be easily modified and versioned.

# Please refer to ramalama.conf(5) for details of all configuration options.
# Not all container engines implement all of the options.
# All of the options have hard coded defaults and these options override
# the built in defaults. Users can override these options via the command
# line. Container engines read ramalama.conf files in up to three
# locations in the following order:
#  1. /usr/share/ramalama/ramalama.conf
#  2. /etc/ramalama/ramalama.conf
#  3. $XDG_CONFIG_HOME/ramalama/ramalama.conf or
#     $HOME/.config/ramalama/ramalama.conf if $XDG_CONFIG_HOME is not set
#  Items specified in the latter ramalama.conf, if they exist, override the
# previous ramalama.conf settings, or the default settings.

[ramalama]

# unified API layer for for Inference, RAG, Agents, Tools, Safety, Evals, and Telemetry.
# Options: llama-stack, none
#
# api = "none"

# OpenAI-compatible API key.
# Can also be set via the RAMALAMA_API_KEY environment variable.
# If both are set, the config value here takes precedence over the environment variable.
#api_key = ""

# OCI model car image
# Image to use when building and pushing --type=car models
#
#carimage = "registry.access.redhat.com/ubi10-micro:latest"

# Run RamaLama in the default container.
#
#container = true

#Min chunk size to attempt reusing from the cache via KV shifting
#
#cache_reuse=256

#size of the prompt context (0 = loaded from model)
#
#ctx_size=0

# Run RamaLama using the specified container engine.
#
# Valid options (Podman, Docker)
#engine = "podman"

# Environment variables to be added when running model within a container
#
#env = []

# Specify the default quantization used when creating OCI formatted AI Models.
# Options: Q2_K, Q3_K_S, Q3_K_M, Q3_K_L, Q4_0, Q4_K_S, Q4_K_M, Q5_0, Q5_K_S, Q5_K_M, Q6_K, Q8_0.
#
#gguf_quantization_mode = "Q4_K_M"

# OCI container image to run with the specified AI model
#
#image = "quay.io/ramalama/ramalama:latest"

# Alternative images to use when RamaLama recognizes specific hardware,
# or user specified vllm model runtime.
#
#[ramalama.images]
#ASAHI_VISIBLE_DEVICES="quay.io/ramalama/asahi"
#ASCEND_VISIBLE_DEVICES="quay.io/ramalama/cann"
#CUDA_VISIBLE_DEVICES="quay.io/ramalama/cuda"
#HIP_VISIBLE_DEVICES="quay.io/ramalama/rocm"
#INTEL_VISIBLE_DEVICES="quay.io/ramalama/intel-gpu"
#MUSA_VISIBLE_DEVICES="quay.io/ramalama/musa"
#VLLM="registry.redhat.io/rhelai1/ramalama-vllm"

# IP address for llama.cpp to listen on.
#
#host = "0.0.0.0"

# Pass `--group-add keep-groups` to podman, when using podman.
# In some cases this is needed to access the gpu from a rootless container
#
#keep_groups = false

# Default number of layers offloaded to the gpu
# -1 means use whatever is automatically deemed appropriate (0 or 999)
#
#ngl = -1

# Specify default port for services to listen on
#
#port = "8080"

# Specify default prefix for chat and run command. By default the prefix
# is based on the container engine used.
# Podman:           "🦭 > "
# Docker:           "🐋 > "
# No Engine:        "🦙 > "
# No IMOGI support: "> "
#
#
#prefix = ""

# Specify default pull policy for OCI Images
#
# **always**: Always pull the image and throw an error if the pull fails.
# **missing**: Only pull the image when it does not exist in the local containers storage.  Throw an error if no image is found and the pull fails.
# **never**: Never pull the image but use the one from the local containers storage.  Throw an error when no image is found.
# **newer**: Pull if the image on the registry is newer than the one in the local containers storage.  An image is considered to be newer when the digests are different.  Comparing the time stamps is prone to errors.  Pull errors are suppressed if a local image was found.
#
#pull = "newer"

# Specify the default output format for output of the `ramalama rag` command
# Options: json, markdown, qdrant
#
#rag_format = "qdrant"

# Specify the AI runtime to use; valid options are 'llama.cpp', 'vllm', and 'mlx' (default: llama.cpp)
# Options: llama.cpp, vllm, mlx
#
#runtime = "llama.cpp"

# SELinux container separation enforcement
#
#selinux = false

# Store AI Models in the specified directory
#
#store = "$HOME/.local/share/ramalama"

# Temperature of the response from the AI Model
# llama.cpp explains this as:
#
#    The lower the number is, the more deterministic the response is.
#
#    The higher the number is the more creative the response, but more likely to hallucinate when set too high.
#
#        Usage: Lower numbers are good for virtual assistants where we need deterministic responses. Higher numbers are good for roleplay or creative tasks like editing stories
#temp=0.8

# Enable thinking mode on reasoning models
#
#thinking = true

# Maximum number of cpu threads to use for inferencing
# -1 will defer to the underlying implementation
#
#threads = -1

# Specify the default transport to be used for pulling and pushing of AI Models.
# Options: oci, ollama, huggingface.
#
#transport = "ollama"
