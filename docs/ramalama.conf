# The RamaLama configuration file specifies all of the available configuration
# command-line options/flags for container engine tools like Podman & Buildah,
# but in a TOML format that can be easily modified and versioned.

# Please refer to ramalama.conf(5) for details of all configuration options.
# Not all container engines implement all of the options.
# All of the options have hard coded defaults and these options override
# the built in defaults. Users can override these options via the command
# line. Container engines read ramalama.conf files in up to three
# locations in the following order:
#  1. /usr/share/ramalama/ramalama.conf
#  2. /etc/ramalama/ramalama.conf
#  3. $XDG_CONFIG_HOME/ramalama/ramalama.conf or
#     $HOME/.config/ramalama/ramalama.conf if $XDG_CONFIG_HOME is not set
#  Items specified in the latter ramalama.conf, if they exist, override the
# previous ramalama.conf settings, or the default settings.

[ramalama]

# unified API layer for for Inference, RAG, Agents, Tools, Safety, Evals, and Telemetry.
# Options: llama-stack, none
#
# api = "none"

# OpenAI-compatible API key.
# Can also be set via the RAMALAMA_API_KEY environment variable.
# If both are set, the config value here takes precedence over the environment variable.
#api_key = ""

# OCI model car image
# Image to use when building and pushing --type=car models
#
#carimage = "registry.access.redhat.com/ubi10-micro:latest"

# Convert the MODEL to the specified OCI Object
# Options: artifact, car, raw
#
# artifact: Store AI Models as artifacts
# car:      Traditional OCI image including base image with the model stored in a /models subdir
# raw:      Traditional OCI image including only the model and a link file `model.file` pointed at it stored at /
#convert_type = "raw"

# Run RamaLama in the default container.
#
#container = true

#Min chunk size to attempt reusing from the cache via KV shifting
#
#cache_reuse=256

#size of the prompt context (0 = loaded from model)
#
#ctx_size=0

# Run RamaLama using the specified container engine.
#
# Valid options (Podman, Docker)
#engine = "podman"

# Environment variables to be added when running model within a container
#
#env = []

# Specify the default quantization used when creating OCI formatted AI Models.
# Options: Q2_K, Q3_K_S, Q3_K_M, Q3_K_L, Q4_0, Q4_K_S, Q4_K_M, Q5_0, Q5_K_S, Q5_K_M, Q6_K, Q8_0.
#
#gguf_quantization_mode = "Q4_K_M"

# OCI container image to run with the specified AI model
#
#image = "quay.io/ramalama/ramalama:latest"

# Alternative images to use when RamaLama recognizes specific hardware,
# or user specified vllm model runtime.
#
#[ramalama.images]
#ASAHI_VISIBLE_DEVICES="quay.io/ramalama/asahi"
#ASCEND_VISIBLE_DEVICES="quay.io/ramalama/cann"
#CUDA_VISIBLE_DEVICES="quay.io/ramalama/cuda"
#HIP_VISIBLE_DEVICES="quay.io/ramalama/rocm"
#INTEL_VISIBLE_DEVICES="quay.io/ramalama/intel-gpu"
#MUSA_VISIBLE_DEVICES="quay.io/ramalama/musa"
#VLLM_ASAHI_VISIBLE_DEVICES="docker.io/vllm/vllm-openai"
#VLLM_ASCEND_VISIBLE_DEVICES="docker.io/vllm/vllm-openai"
#VLLM_CUDA_VISIBLE_DEVICES="docker.io/vllm/vllm-openai"
#VLLM_GGML_VK_VISIBLE_DEVICES="docker.io/vllm/vllm-openai"
#VLLM_HIP_VISIBLE_DEVICES="docker.io/vllm/vllm-openai"
#VLLM_INTEL_VISIBLE_DEVICES="docker.io/vllm/vllm-openai"
#VLLM_MUSA_VISIBLE_DEVICES="docker.io/vllm/vllm-openai"

# IP address for llama.cpp to listen on.
#
#host = "0.0.0.0"

# Pass `--group-add keep-groups` to podman, when using podman.
# In some cases this is needed to access the gpu from a rootless container
#
#keep_groups = false

# Set the logging level of RamaLama application.
# Valid Values:
#    debug, info, warning, error, critical
# Note: --debug option overrides this field and forces the system to debug
#log_level=warning

# Maximum number of tokens to generate. Set to 0 for unlimited output
# This parameter is mapped to the appropriate runtime-specific parameter
# when executing models.
#
#max_tokens=0

# Default number of layers offloaded to the gpu
# -1 means use whatever is automatically deemed appropriate (0 or 999)
#
#ngl = -1

# Specify initial port for a range of 100 ports for services to listen on.
# If the specified port is 8080, a free port in the 8080-8179 range (100 ports) is selected.
#
#port = "8080"

# Specify default prefix for chat and run command. By default the prefix
# is based on the container engine used.
# Podman:           "ðŸ¦­ > "
# Docker:           "ðŸ‹ > "
# No Engine:        "ðŸ¦™ > "
# No IMOGI support: "> "
#
#
#prefix = ""

# Specify default pull policy for OCI Images
#
# **always**: Always pull the image and throw an error if the pull fails.
# **missing**: Only pull the image when it does not exist in the local containers storage.  Throw an error if no image is found and the pull fails.
# **never**: Never pull the image but use the one from the local containers storage.  Throw an error when no image is found.
# **newer**: Pull if the image on the registry is newer than the one in the local containers storage.  An image is considered to be newer when the digests are different.  Comparing the time stamps is prone to errors.  Pull errors are suppressed if a local image was found.
#
#pull = "newer"

# Specify the default output format for output of the `ramalama rag` command
# Options: qdrant, json, markdown, milvus
#
#rag_format = "qdrant"

# Alternative RAG images to use when RamaLama recognizes specific hardware,
#
#[ramalama.rag_images]
#CUDA_VISIBLE_DEVICES="quay.io/ramalama/cuda-rag"
#HIP_VISIBLE_DEVICES="quay.io/ramalama/rocm-rag"
#INTEL_VISIBLE_DEVICES="quay.io/ramalama/intel-gpu-rag"

# Specify the AI runtime to use; valid options are 'llama.cpp', 'vllm', and 'mlx' (default: llama.cpp)
# Options: llama.cpp, vllm, mlx
#
#runtime = "llama.cpp"

# SELinux container separation enforcement
#
#selinux = false

# Store AI Models in the specified directory
#
#store = "$HOME/.local/share/ramalama"

# Automatically summarize conversation history after N messages to prevent context growth
# When enabled, ramalama will periodically condense older messages into a summary,
# keeping only recent messages and the summary. This prevents the context from growing
# indefinitely during long chat sessions. Set to 0 to disable.
#
#summarize_after = 4

# Temperature of the response from the AI Model
# llama.cpp explains this as:
#
#    The lower the number is, the more deterministic the response is.
#
#    The higher the number is the more creative the response, but more likely to hallucinate when set too high.
#
#        Usage: Lower numbers are good for virtual assistants where we need deterministic responses. Higher numbers are good for roleplay or creative tasks like editing stories
#temp=0.8

# Enable thinking mode on reasoning models
#
#thinking = true

# Maximum number of cpu threads to use for inferencing
# -1 will defer to the underlying implementation
#
#threads = -1

# Specify the default transport to be used for pulling and pushing of AI Models.
# Options: oci, ollama, huggingface.
#
#transport = "ollama"

# Http client configuration
#
#[ramalama.http_client]
#
# The maximum number of times to retry a failed download.
#
#max_retries = 5
#
# The maximum delay between retry attempts in seconds.
#
#max_retry_delay = 30
# Suppress the interactive prompt when running on macOS with a Podman VM
# that doesn't support GPU acceleration (e.g., applehv provider).
# When set to true, RamaLama will automatically proceed without GPU support
# instead of asking for confirmation.
# Can also be set via the `RAMALAMA_USER__NO_MISSING_GPU_PROMPT` environment variable.
#

[ramalama.user]
#no_missing_gpu_prompt = false
