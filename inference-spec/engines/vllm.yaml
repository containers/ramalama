schema_version: "1.0.0"
commands:
  - name: serve
    inference_engine:
      name: "vllm server"
      binary: "/opt/venv/bin/python3 -m vllm.entrypoints.openai.api_server"
      options: &serve_run_options
        - name: "--model"
          description: "The AI model to run"
          value: "{{ model.model_path }}"
        - name: "--served-model-name"
          description: "The name assigned to the run AI model"
          value: "{{ model.model_name }}"
        - name: "--max_model_len"
          description: "Size of the model context"
          value: "{{ args.ctx_size if args.ctx_size else 2048 }}"
        - name: "--port"
          description: "Port for the AI model server to listen on"
          value: "{{ args.port }}"
        - name: "--seed"
          description: "Seed the global PRNG"
          value: "{{ args.seed }}"
        # Special case:
        # Pass arbitrary runtime arguments to llama-server
        - name: ""
          description: "Arbitrary runtime arguments for llama-server"
          value: "{{ args.runtime_args }}"
  - name: run
    inference_engine:
      name: "vllm server with chat"
      binary: "vllm start"
      options: *serve_run_options
