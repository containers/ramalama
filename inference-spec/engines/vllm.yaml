schema_version: "1.0.0"
commands:
  - name: serve
    inference_engine:
      name: "vllm server"
      binary: "/opt/venv/bin/python3 -m vllm.entrypoints.openai.api_server"
      options: &serve_run_options
        - name: "--model"
          description: "The AI model to run"
          value: "{{ model.model_path }}"
        - name: "--served-model-name"
          description: "The name assigned to the run AI model"
          value: "{{ model.model_name }}"
        - name: "--max_model_len"
          description: "Size of the model context"
          value: "{{ args.ctx_size if args.ctx_size else 2048 }}"
        - name: "--port"
          description: "Port for the AI model server to listen on"
          value: "{{ args.port }}"
        - name: "--seed"
          description: "Seed the global PRNG"
          value: "{{ args.seed }}"
        # Special case:
        # Pass arbitrary runtime arguments to llama-server
        - name: ""
          description: "Arbitrary runtime arguments for llama-server"
          value: "{{ args.runtime_args }}"
  - name: run
    inference_engine:
      name: "vllm server with chat"
      binary: "vllm start"
      options: *serve_run_options
  - name: rag
    inference_engine:
      name: "rag"
      binary: "doc2rag"
      options:
        - name: "--debug"
          description: "Enable debug logging"
          if: "{{ args.debug }}"
        - name: "--format"
          description: "Output format for RAG data"
          value: "{{ args.format }}"
        - name: "--ocr"
          description: "Enable embedded image text extraction from PDF"
          if: "{{ args.ocr }}"
        - name: "/output"
          description: "Destination directory for RAG data"
        - name: ""
          description: "Directory containing input files for processing"
          value: "{{ args.inputdir }}"
          if: "{{ args.paths }}"
        - name: ""
          description: "URLs to process"
          value: "{{ args.urls }}"
  - name: convert
    inference_engine:
      name: "convert hf to gguf"
      binary: "convert_hf_to_gguf.py"
      options:
        - name: "--outfile"
          description: "Output file for the converted model"
          value: "/output/{{ model.model_name }}.gguf"
        - name: ""
          description: "Path to the model to convert"
          value: "/model"
