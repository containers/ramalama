ARG CUDA_VERSION=12.9.1
# Base image with CUDA for compilation
FROM docker.io/nvidia/cuda:${CUDA_VERSION}-devel-ubi9 AS builder

COPY container-images/scripts/build_llama.sh \
     container-images/scripts/lib.sh \
     /src/
WORKDIR /src/
RUN ./build_llama.sh cuda

# Final runtime image
FROM docker.io/nvidia/cuda:${CUDA_VERSION}-runtime-ubi9

RUN --mount=type=bind,from=builder,source=/tmp/install,target=/tmp/install \
    cp -a /tmp/install/bin/ /usr/ && \
    cp -a /tmp/install/lib64/*.so* /usr/lib64/
RUN --mount=type=bind,target=/src \
    /src/container-images/scripts/build_llama.sh cuda runtime

# Install ramalama to support a non-standard use-case
RUN --mount=type=bind,target=/src \
    cp -a /src /var/tmp/ramalama && \
    python3 -m pip \
      --no-cache-dir \
      --disable-pip-version-check \
      install \
        --compile \
        --prefix=/usr \
        --root-user-action ignore \
        /var/tmp/ramalama && \
    rm -rf /var/tmp/ramalama

WORKDIR /
ENTRYPOINT []
CMD ["/bin/bash"]
