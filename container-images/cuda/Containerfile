ARG VERSION=12.8.1
# Base image with CUDA for compilation
FROM docker.io/nvidia/cuda:${VERSION}-devel-ubi9 AS builder

COPY . /src/ramalama
WORKDIR /src/ramalama
RUN container-images/scripts/build_llama_and_whisper.sh cuda
RUN container-images/scripts/build_vllm.sh "cuda"

# Final runtime image
FROM docker.io/nvidia/cuda:${VERSION}-runtime-ubi9

# Copy the entire installation directory from the builder
COPY --from=builder /tmp/install /usr

# Copy vllm related stuff
COPY --from=builder /usr/local/lib64 /usr/local/lib64
COPY --from=builder /usr/local/lib /usr/local/lib
COPY --from=builder /usr/local/bin /usr/local/bin
RUN dnf install -y cuda-cupti-12-8 libcudnn9-devel-cuda-12 gcc
ENV LD_LIBRARY_PATH=/usr/local/lib/python3.11/site-packages/cusparselt/lib:/usr/local/cuda-12.8/targets/x86_64-linux/lib/

# Workaround for CUDA libraries not in the ld path in base container
RUN echo "/usr/local/cuda-12.8/compat" > /etc/ld.so.conf.d/99_cuda_compat.conf && ldconfig

RUN dnf install -y python3.11 python3.11-pip python3.11-devel && \
    dnf -y clean all && \
    ln -sf /usr/bin/python3.11 /usr/bin/python3

ENTRYPOINT []
