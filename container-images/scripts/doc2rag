#!/usr/bin/env python3

import argparse
import errno
import hashlib
import os
import os.path
import sys
import uuid

import docling
import qdrant_client
from docling.chunking import HybridChunker
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from qdrant_client import models

# Global Vars
EMBED_MODEL = os.getenv("EMBED_MODEL", "jinaai/jina-embeddings-v2-small-en")
SPARSE_MODEL = os.getenv("SPARSE_MODEL", "prithivida/Splade_PP_en_v1")
COLLECTION_NAME = "rag"


class Converter:
    """A Class designed to handle all document conversions using Docling"""

    def __init__(self, args):
        # Docling Setup (Turn off OCR (image processing) for drastically reduced RAM usage and big speed increase)
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = args.ocr
        self.sources = []
        for source in args.sources:
            self.add(source)
        self.output = args.output
        self.format = args.format
        self.doc_converter = DocumentConverter(
            format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}
        )
        if self.format == "qdrant":
            self.client = qdrant_client.QdrantClient(path=self.output)
            self.client.set_model(EMBED_MODEL)
            self.client.set_sparse_model(SPARSE_MODEL)
            # optimizations to reduce ram
            self.client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=self.client.get_fastembed_vector_params(on_disk=True),
                sparse_vectors_config=self.client.get_fastembed_sparse_vector_params(on_disk=True),
                quantization_config=models.ScalarQuantization(
                    scalar=models.ScalarQuantizationConfig(
                        type=models.ScalarType.INT8,
                        always_ram=True,
                    ),
                ),
            )

    def add(self, file_path):
        if os.path.isdir(file_path):
            self.walk(file_path)  # Walk directory and process all files
        else:
            self.sources.append(file_path)  # Process the single file

    def convert_qdrant(self, results):
        documents, ids = [], []
        chunker = HybridChunker(tokenizer=EMBED_MODEL, overlap=100, merge_peers=True)
        for result in results:
            chunk_iter = chunker.chunk(dl_doc=result.document)
            for chunk in chunk_iter:
                # Extract the enriched text from the chunk
                doc_text = chunker.contextualize(chunk=chunk)
                # Append to respective lists
                documents.append(doc_text)
                # Generate unique ID for the chunk
                doc_id = self.generate_hash(doc_text)
                ids.append(doc_id)
        return self.client.add(COLLECTION_NAME, documents=documents, ids=ids, batch_size=1)

    def convert(self):
        results = self.doc_converter.convert_all(self.sources)
        if self.format == "qdrant":
            return self.convert_qdrant(results)
        if self.format == "markdown":
            # Export the converted document to Markdown
            return self.convert_markdown(results)
        if self.format == "json":
            # Export the converted document to JSON
            return self.convert_json(results)

    def convert_markdown(self, results):
        ctr = 0
        # Process the conversion results
        for ctr, result in enumerate(results):
            dirname = self.output + os.path.dirname(self.sources[ctr])
            os.makedirs(dirname, exist_ok=True)
            document = result.document
            document.save_as_markdown(os.path.join(dirname, f"{document.name}.md"))

    def convert_json(self, results):
        for ctr, result in enumerate(results):
            dirname = self.output + os.path.dirname(self.sources[ctr])
            os.makedirs(dirname, exist_ok=True)
            document = result.document
            document.save_as_json(os.path.join(dirname, f"{document.name}.json"))

    def walk(self, path):
        for root, dirs, files in os.walk(path, topdown=True):
            if len(files) == 0:
                continue
            for f in files:
                file = os.path.join(root, f)
                if os.path.isfile(file):
                    self.sources.append(file)

    def generate_hash(self, document: str) -> str:
        """Generate a unique hash for a document."""
        sha256_hash = hashlib.sha256(document.encode('utf-8')).hexdigest()

        # Use the first 32 characters of the hash to create a UUID
        return str(uuid.UUID(sha256_hash[:32]))


def load():
    # Dummy code to preload models
    client = qdrant_client.QdrantClient(":memory:")
    client.set_model(EMBED_MODEL)
    client.set_sparse_model(SPARSE_MODEL)
    converter = DocumentConverter()
    converter.initialize_pipeline(InputFormat.PDF)


parser = argparse.ArgumentParser(
    prog="docling",
    description="process source files into RAG vector database",
)

parser.add_argument("output", nargs="?", help="Output directory")
parser.add_argument("sources", nargs="*", help="Source files")
parser.add_argument(
    "--format",
    default="qdrant",
    help="Output format for RAG Data",
    choices=["qdrant", "json", "markdown"],
)
parser.add_argument(
    "--ocr",
    action='store_true',
    help="Enable embedded image text extraction from PDF (Increases RAM Usage significantly)",
)


def perror(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)


def eprint(e, exit_code):
    perror("Error: " + str(e).strip("'\""))
    sys.exit(exit_code)


try:
    args = parser.parse_args()
    if args.output == "load":
        load()
    else:
        converter = Converter(args)
        converter.convert()
except docling.exceptions.ConversionError as e:
    eprint(e, 1)
except FileNotFoundError as e:
    eprint(e, errno.ENOENT)
except ValueError as e:
    eprint(e, errno.EINVAL)
except KeyboardInterrupt:
    pass
