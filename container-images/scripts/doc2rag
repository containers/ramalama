#!/usr/bin/env python3

import argparse
import hashlib
import os
import sys
import uuid
import gc
from pypdf import PdfReader, PdfWriter
import tempfile

import qdrant_client
from qdrant_client import models
import docling
from docling.chunking import HybridChunker
from docling.document_converter import DocumentConverter

# Global Vars
EMBED_MODEL = "jinaai/jina-embeddings-v2-small-en"
SPARSE_MODEL = "prithivida/Splade_PP_en_v1"
COLLECTION_NAME = "rag"

class Converter:
    """A Class designed to handle all document conversions using Docling"""

    def __init__(self, output, targets):
        self.doc_converter = DocumentConverter()
        self.targets = []
        for target in targets:
            self.add(target)
        self.output = output
        self.client = qdrant_client.QdrantClient(path=output)
        self.client.set_model(EMBED_MODEL)
        self.client.set_sparse_model(SPARSE_MODEL)
        # optimizations to reduce ram
        self.client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=self.client.get_fastembed_vector_params(on_disk=True),
            sparse_vectors_config= self.client.get_fastembed_sparse_vector_params(on_disk=True),
        )

    def add(self, file_path):
        if os.path.isdir(file_path):
            self.walk(file_path)  # Walk directory and process all files
        else:
            self.targets.append(file_path)  # Process the single file


    def convert(self):
        chunker = HybridChunker(tokenizer=EMBED_MODEL, overlap=100, merge_peers=True)

        for path in self.targets:
            if path.lower().endswith(".pdf") and self._pdf_page_count(path) > 20:
                print(f"Streaming large PDF page-by-page: {path}")
                for page_path in self._page_iterator(path):
                    print("Converting page")
                    result = DocumentConverter().convert(page_path)
                    self._process_doc(result, chunker)
                    print("done")
                    del result
                    gc.collect()
            else:
                result = self.doc_converter.convert(path)
                self._process_doc(result, chunker)
                gc.collect()


    def _process_doc(self, result, chunker):
        chunk_iter = chunker.chunk(dl_doc=result.document)
        for chunk in chunk_iter:
            doc_text = chunker.contextualize(chunk=chunk)
            print(doc_text)
            doc_id = self.generate_hash(doc_text)
            self.client.add(COLLECTION_NAME, documents=[doc_text], ids=[doc_id], batch_size=1)

    def _pdf_page_count(self, path):
        with open(path, "rb") as f:
            reader = PdfReader(f)
            return len(reader.pages)

    def _page_iterator(self, path):
        with open(path, "rb") as f:
            reader = PdfReader(f)
            for i, page in enumerate(reader.pages):
                print("page: ", i+1)
                writer = PdfWriter()
                writer.add_page(page)
                temp_fd, temp_path = tempfile.mkstemp(suffix=f"_page_{i}.pdf")
                with os.fdopen(temp_fd, 'wb') as tmp_file:
                    writer.write(tmp_file)
                yield temp_path
                os.remove(temp_path) 


    def walk(self, path):
        for root, dirs, files in os.walk(path, topdown=True):
            if len(files) == 0:
                continue
            for f in files:
                file = os.path.join(root, f)
                if os.path.isfile(file):
                    self.targets.append(file)

    def generate_hash(self, document: str) -> str:
        """Generate a unique hash for a document."""
        sha256_hash = hashlib.sha256(document.encode('utf-8')).hexdigest()

        # Use the first 32 characters of the hash to create a UUID
        return str(uuid.UUID(sha256_hash[:32]))

def load():
    # Dummy code to preload models
    client = qdrant_client.QdrantClient(":memory:")
    client.set_model(EMBED_MODEL)
    client.set_sparse_model(SPARSE_MODEL)

parser = argparse.ArgumentParser(
    prog="docling",
    description="process source files into RAG vector database",
)

parser.add_argument("target", nargs="?", help="Target database")
parser.add_argument("source", nargs="*", help="Source files")

def perror(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)


def eprint(e, exit_code):
    perror("Error: " + str(e).strip("'\""))
    sys.exit(exit_code)

try:
    args = parser.parse_args()
    if args.target == "load":
        load()
    else:
        converter = Converter(args.target, args.source)
        converter.convert()
except docling.exceptions.ConversionError as e:
    eprint(e, 1)
except FileNotFoundError as e:
    eprint(e, 1)
except KeyboardInterrupt:
    pass